{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Anomaly_Detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnTO4SempolapI/PCG03ry",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/warlmare/TCPcorrelator/blob/master/Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkcm5t5hfc3M",
        "outputId": "48356dcd-7cf5-4a1c-fe90-b8fd31b5a7c2"
      },
      "source": [
        "!pip install sgt\n",
        "!pip install py-tlsh\n",
        "\n",
        "from datetime import datetime\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import tlsh\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sgt\n",
            "  Downloading sgt-2.0.3-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: sgt\n",
            "Successfully installed sgt-2.0.3\n",
            "Collecting py-tlsh\n",
            "  Downloading py-tlsh-4.7.2.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 426 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: py-tlsh\n",
            "  Building wheel for py-tlsh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-tlsh: filename=py_tlsh-4.7.2-cp37-cp37m-linux_x86_64.whl size=70230 sha256=c948b0e8f62013e600a4db6e85ef6c6ca849e180438ad7069e4f5d61fb8d51af\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/f8/a4/99e9c89728dbb9550dc11901389858dfcb287089e5887e8cd6\n",
            "Successfully built py-tlsh\n",
            "Installing collected packages: py-tlsh\n",
            "Successfully installed py-tlsh-4.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAMEMsa8dL3v"
      },
      "source": [
        "class SimpleClassificationModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocabulary_size):\n",
        "        \"\"\"\n",
        "        .\n",
        "        \"\"\"\n",
        "        super(SimpleClassificationModel, self).__init__()\n",
        "\n",
        "        self.char_embedding = nn.Embedding(vocabulary_size, vocabulary_size)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.predictor = nn.Linear(hidden_size, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.char_embedding(x)\n",
        "        x = self.net(x)\n",
        "\n",
        "        # Perform Global average pooling for tensor resizing\n",
        "        x = x.mean(1)\n",
        "\n",
        "        return self.predictor(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPUCUcdZJ8tG"
      },
      "source": [
        "generating \"normal\" and \"anomalous\" data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZpMOtlDfF8d"
      },
      "source": [
        "n_training = 10_000\n",
        "n_testing = 2_000\n",
        "\n",
        "S = 100\n",
        "anomaly_len = 30\n",
        "anomaly = \"_\" * anomaly_len\n",
        "\n",
        "def get_normal_hash():\n",
        "    ran_a = ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(S))\n",
        "    ran_tlsh_a = tlsh.hash(str.encode(ran_a))\n",
        "    return ran_tlsh_a\n",
        " \n",
        "def get_anom_hash():\n",
        "    ran_b = ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(S))\n",
        "    pos = random.randint(anomaly_len,S - anomaly_len)\n",
        "    new_str = ran_b[:pos] + anomaly + ran_b[pos+anomaly_len:]\n",
        "    ran_tlsh_b = tlsh.hash(str.encode(new_str))\n",
        "    return ran_tlsh_b\n",
        "\n",
        "# Note: You need balanced training data.\n",
        "training_data_normal = [get_normal_hash() for i in range(n_training // 2)]\n",
        "training_data_anom = [get_anom_hash() for i in range(n_training // 2)]\n",
        "training_data_list = training_data_normal + training_data_anom\n",
        "\n",
        "# Assuming that anomalie is [0, 1] and normal is [1, 0].\n",
        "training_labels = [[1, 0] for i in range(n_training // 2)] + [[0, 1] for i in range(n_training // 2)]\n",
        "\n",
        "# Also for testing having balanced data makes interpretation easier.\n",
        "test_data_normal = [get_normal_hash() for i in range(n_testing // 2)]\n",
        "test_data_anom = [get_anom_hash() for i in range(n_testing // 2)]\n",
        "test_data_list = test_data_normal + test_data_anom\n",
        "\n",
        "# Assuming that anomalie is [0, 1] and normal is [1, 0].\n",
        "test_labels = [[1, 0] for i in range(n_testing // 2)] + [[0, 1] for i in range(n_testing // 2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCl2yqdfP_J"
      },
      "source": [
        "#### Pytorch Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQRaFBlfUu8"
      },
      "source": [
        "class HashDataset(Dataset):\n",
        "    \"\"\"Freedoms fancy hash dataset with anomalies\"\"\"\n",
        "\n",
        "    def __init__(self, hashes, labels, all_chars, transform=False):\n",
        "        \"\"\"\n",
        "        .\n",
        "        \"\"\"\n",
        "        super(HashDataset, self).__init__()\n",
        "\n",
        "        self._vocab_size = len(all_chars)\n",
        "\n",
        "        self.char_to_int = dict((c, i) for i, c in enumerate(all_chars))\n",
        "        self.int_to_char = dict((i, c) for i, c in enumerate(all_chars))\n",
        "\n",
        "        self.hashes = hashes\n",
        "        self.labels = labels\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.data_min = 0\n",
        "        self.data_max = len(all_chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hashes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        inputs = [self.char_to_int[char] for char in self.hashes[idx]]\n",
        "        label = torch.FloatTensor(self.labels[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            inputs_scaled = inputs / self.data_max\n",
        "            return torch.FloatTensor(inputs_scaled), label\n",
        "        else:\n",
        "            return torch.LongTensor(inputs), label\n",
        "\n",
        "    def decode_hash(self, encoded_hash):\n",
        "        return ''.join(self.int_to_char[_int] for _int in encoded_hash)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self._vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0HDLQi_hCHv"
      },
      "source": [
        "### Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmBYhR6phFdq"
      },
      "source": [
        "print_every = 50\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "max_steps = 50000\n",
        "learning_rate = 0.01\n",
        "batch_size = 512\n",
        "hidden_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzN_YbrchJaN"
      },
      "source": [
        "### Create Datasets and Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cksevfenlT"
      },
      "source": [
        "ALL_CHARS = '0abcdefghijklmnopqrstuvwxyz'\n",
        "ALL_CHARS +='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "ALL_CHARS += '123456789'\n",
        "ALL_CHARS += '().,-/+=&$?@#!*:;_[]|%⸏{}\\\"\\'' + ' ' +'\\\\'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApwSYpF5hOnb"
      },
      "source": [
        "train_dataset = HashDataset(\n",
        "    hashes=training_data_list,\n",
        "    labels=training_labels,\n",
        "    all_chars=ALL_CHARS\n",
        ")\n",
        "dataset_size = len(train_dataset)\n",
        "vocabulary_size = train_dataset.vocab_size\n",
        "input_size = vocabulary_size\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)\n",
        "train_loader_generator = iter(train_data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s82cqfVs8M9m"
      },
      "source": [
        "generating a separate test-data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_PO1X2O8R8m"
      },
      "source": [
        "test_dataset = HashDataset(\n",
        "    hashes = test_data_list,\n",
        "    labels = test_labels,\n",
        "    all_chars=ALL_CHARS\n",
        ")\n",
        "dataset_size = len(test_dataset)\n",
        "vocabulary_size = test_dataset.vocab_size\n",
        "input_size = vocabulary_size\n",
        "\n",
        "test_data_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=2)\n",
        "test_loader_generator = iter(test_data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBng213ThSwR"
      },
      "source": [
        "### Initialize Models and Optimizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCepAsaZhWfo"
      },
      "source": [
        "# Initialize the model that we are going to use\n",
        "model = SimpleClassificationModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=hidden_size,\n",
        "    vocabulary_size=vocabulary_size\n",
        ")\n",
        "print(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Setup the loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hymufwXthg1s"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcGhfjrmhfHF"
      },
      "source": [
        "# main training loop\n",
        "for step in range(max_steps):\n",
        "\n",
        "    try:\n",
        "        # Samples the batch\n",
        "        batch_inputs, batch_labels = next(train_loader_generator)\n",
        "    except StopIteration:\n",
        "        # restart the generator if the previous generator is exhausted.\n",
        "        train_loader_generator = iter(train_data_loader)\n",
        "        batch_inputs, batch_labels = next(train_loader_generator)\n",
        "\n",
        "    batch_inputs = batch_inputs.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    prediction_logits = model(batch_inputs)\n",
        "\n",
        "    loss = criterion(prediction_logits, batch_labels)\n",
        "    #accuracy would be nice\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % print_every == 0:\n",
        "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M')}], Step = {step}/{max_steps}, Loss = {loss}\")\n",
        "         \n",
        "print('Done training.')\n",
        "torch.save(model, \"trained_model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Uu1IgdRtHeSi",
        "outputId": "8982a36b-1660-4489-d999-dd824dd7fcad"
      },
      "source": [
        "\n",
        "\n",
        "model = torch.load(\"trained_model.pth\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "Batched_inputs =  next(iter(test_data_loader))\n",
        "print(len(Batched_inputs))\n",
        "\n",
        "inputs, labels = Batched_inputs\n",
        "print(type(inputs))\n",
        "\n",
        "print(inputs[0])\n",
        "print(labels[0])\n",
        "\n",
        "\n",
        "\n",
        "#debug_loader = torch.utils.data.DataLoader(debug_data, batch_size=128)\n",
        "\n",
        "#model(debug_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model expects (128 x 2) (batchsize x features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c61ae4763a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    136\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    }
  ]
}